# Fast Learning

A full-stack conversational AI learning platform built with Flask and modern web technologies. Features real-time chat with WebSocket support, conversation history, universal knowledge system, and multiple AI model support.

![Fast Learning](https://img.shields.io/badge/AI-Fast%20Learning-10a37f?style=for-the-badge)
![Python](https://img.shields.io/badge/Python-3.12-blue?style=for-the-badge)
![Flask](https://img.shields.io/badge/Flask-3.0.0-black?style=for-the-badge)
![WebSocket](https://img.shields.io/badge/WebSocket-Enabled-orange?style=for-the-badge)

## âœ¨ Features

### ğŸ¤– AI Capabilities
- **Multiple AI Models**: Support for GPT-3.5, GPT-4, and local Hugging Face models
- **Real-time Streaming**: See AI responses as they're generated
- **Context-Aware**: Maintains conversation history for coherent responses
- **Smart Fallback**: Automatically switches between OpenAI, local models, and rule-based responses

### ğŸ’¬ Chat Features
- **Modern AI Chat Interface**: Clean, intuitive interface with gradient design
- **Conversation Management**: Save, load, and delete chat sessions
- **Message History**: Persistent storage of all conversations
- **Typing Indicators**: Real-time feedback during AI response generation
- **Code Highlighting**: Syntax highlighting for code snippets
- **Markdown Support**: Formatted text with bold, italic, and code blocks

### ğŸ¨ User Experience
- **Dark/Light Theme**: Toggle between themes with persistence
- **Responsive Design**: Works on desktop, tablet, and mobile
- **WebSocket Support**: Real-time bidirectional communication
- **Example Prompts**: Quick-start suggestions for new users
- **Session Persistence**: Conversations saved automatically

## ğŸš€ Quick Start

### Prerequisites
- Python 3.12+
- Virtual environment (recommended)
- Optional: OpenAI API key for GPT models

### Installation

1. **Clone or navigate to the repository**
```bash
cd fast-learning
```

2. **Create and activate virtual environment**
```bash
python3 -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

3. **Install dependencies**
```bash
pip install -r requirements.txt
```

4. **Optional: Set OpenAI API key** (for GPT models)
```bash
export OPENAI_API_KEY='your-api-key-here'
```
Or create a `.env` file:
```
OPENAI_API_KEY=your-api-key-here
```

5. **Run the application**
```bash
cd backend
python app.py
```

6. **Open in browser**
```
http://localhost:5000
```

## ğŸ“ Project Structure

```
fast-learning/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ app.py                      # Flask application with WebSocket
â”‚   â”œâ”€â”€ conversations/              # Stored conversation JSON files
â”‚   â”œâ”€â”€ models/                     # AI model configurations
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ ai_engine.py           # AI model integration
â”‚       â””â”€â”€ conversation_manager.py # Conversation storage logic
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ index.html                 # Main HTML structure
â”‚   â”œâ”€â”€ css/
â”‚   â”‚   â””â”€â”€ style.css             # Modern UI styling
â”‚   â””â”€â”€ js/
â”‚       â””â”€â”€ app.js                # Frontend logic & WebSocket client
â”œâ”€â”€ data/                          # Additional data storage
â”œâ”€â”€ requirements.txt               # Python dependencies
â”œâ”€â”€ README.md                      # This file
â”œâ”€â”€ QUICKSTART.md                  # Quick setup guide
â””â”€â”€ LICENSE                        # MIT License
```

## ğŸŒ Deploy to Cloud (Get Your Public URL)

Your Fast Learning platform can be deployed to the cloud and accessible from anywhere with a public URL like `https://your-app.onrender.com`.

### Option 1: Deploy to Render.com (Recommended - FREE)

Render offers free hosting with automatic HTTPS and custom domains.

#### Steps:

1. **Push your code to GitHub**
   ```bash
   # If not already initialized
   git init
   git add .
   git commit -m "Ready for deployment"
   
   # Create repository on GitHub, then:
   git remote add origin https://github.com/yourusername/fast-learning.git
   git push -u origin main
   ```

2. **Deploy on Render**
   - Visit [https://render.com](https://render.com) and sign up
   - Click **"New +"** â†’ **"Web Service"**
   - Connect your GitHub repository
   - Render will auto-detect the `render.yaml` configuration
   - Click **"Deploy"**
   - Your app will be live at: `https://fast-learning-xxxx.onrender.com`

3. **Add Environment Variables (Optional)**
   - In Render dashboard, go to **Environment** tab
   - Add: `OPENAI_API_KEY` = `your-api-key` (for GPT models)

#### Features:
- âœ… Free tier available
- âœ… Automatic HTTPS/SSL
- âœ… Custom domain support
- âœ… Auto-deploy from GitHub
- âœ… Zero configuration needed

### Option 2: Deploy to Railway.app (FREE with $5 credit)

Railway provides instant deployments with generous free tier.

#### Steps:

1. **Push to GitHub** (same as above)

2. **Deploy on Railway**
   - Visit [https://railway.app](https://railway.app)
   - Click **"Start a New Project"**
   - Select **"Deploy from GitHub repo"**
   - Choose your `fast-learning` repository
   - Railway auto-detects Python and deploys
   - Get your URL: `https://fast-learning.up.railway.app`

3. **Configure Environment**
   - Click on your service â†’ **Variables** tab
   - Add `OPENAI_API_KEY` if using GPT models

### Option 3: Deploy to Heroku

Heroku is a mature platform with extensive documentation.

#### Steps:

1. **Install Heroku CLI**
   ```bash
   # Ubuntu/Debian
   curl https://cli-assets.heroku.com/install.sh | sh
   
   # macOS
   brew tap heroku/brew && brew install heroku
   ```

2. **Login and Create App**
   ```bash
   heroku login
   heroku create fast-learning-ai
   ```

3. **Deploy**
   ```bash
   git push heroku main
   ```

4. **Set Environment Variables**
   ```bash
   heroku config:set OPENAI_API_KEY=your-api-key
   ```

5. **Open Your App**
   ```bash
   heroku open
   ```
   - URL: `https://fast-learning-ai.herokuapp.com`

### Custom Domain (Optional)

Once deployed, you can add a custom domain like `fastlearning.ai`:

1. **Purchase Domain** (Namecheap, GoDaddy, Google Domains)
2. **Add to Platform**:
   - **Render**: Dashboard â†’ Settings â†’ Custom Domains
   - **Railway**: Project Settings â†’ Domains
   - **Heroku**: `heroku domains:add www.yourname.com`
3. **Configure DNS**:
   - Add CNAME record pointing to your platform's URL
   - SSL certificates are automatic on all platforms

### Production Checklist

Before going live, ensure:
- âœ… `debug=False` in `app.py` (already set)
- âœ… Environment variables configured (OpenAI API key)
- âœ… CORS settings updated for production domain
- âœ… Error logging enabled
- âœ… Database backup strategy (if using database)

## ğŸ”§ Configuration

### AI Models

The platform supports multiple AI providers:

1. **OpenAI (GPT-3.5/GPT-4)**
   - Requires API key in `.env` file
   - Best quality responses
   - Usage charges apply

2. **Hugging Face (Local Models)**
   - Free to use
   - Runs locally (DialoGPT-medium by default)
   - No API key required
   - First run downloads model (~1GB)

3. **Rule-Based Fallback**
   - Always available
   - Pattern-matching responses
   - No setup required

### Environment Variables

Create a `.env` file in the root directory:

```env
# OpenAI Configuration (optional)
OPENAI_API_KEY=your-api-key-here

# Server Configuration
FLASK_ENV=development
FLASK_DEBUG=True
PORT=5000

# Model Settings
DEFAULT_MODEL=gpt-3.5-turbo
MAX_TOKENS=1000
TEMPERATURE=0.7
```

## ğŸŒ API Endpoints

### REST API

- `GET /api/health` - Health check
- `POST /api/chat` - Send message (non-streaming)
- `GET /api/conversations` - List all conversations
- `GET /api/conversations/<id>` - Get specific conversation
- `DELETE /api/conversations/<id>` - Delete conversation
- `POST /api/conversations/clear` - Clear all conversations
- `GET /api/models` - List available AI models

### WebSocket Events

**Client â†’ Server:**
- `send_message` - Send chat message
- `new_conversation` - Create new conversation

**Server â†’ Client:**
- `connected` - Connection established
- `typing` - AI is typing indicator
- `message_chunk` - Streaming response chunk
- `message_complete` - Response finished
- `conversation_created` - New conversation created
- `error` - Error occurred

## ğŸ’¡ Usage Examples

### Basic Chat
1. Open the application in your browser
2. Type a message in the input box
3. Press Enter or click Send
4. Watch the AI response stream in real-time

### Switch AI Models
- Use the dropdown in the header to select different models
- Choose between GPT-3.5, GPT-4, or local models
- Model preference is saved per conversation

### Manage Conversations
- **New Chat**: Click "New Chat" button in sidebar
- **Load Chat**: Click on any conversation in the sidebar
- **Delete Chat**: Hover over conversation and click trash icon
- **Clear All**: Click "Clear All Chats" at bottom of sidebar

### Example Prompts

Try these example prompts:
- "Explain quantum computing in simple terms"
- "Write a Python function to reverse a string"
- "Give me tips for learning AI and machine learning"
- "What are the best practices for web development?"

## ğŸ› ï¸ Technology Stack

### Backend
- **Flask 3.0.0** - Web framework
- **Flask-SocketIO** - WebSocket support
- **OpenAI API** - GPT models integration
- **Transformers** - Hugging Face model support
- **PyTorch** - Deep learning framework
- **NLTK** - Natural language processing

### Frontend
- **HTML5/CSS3** - Modern web standards
- **Vanilla JavaScript** - No framework dependencies
- **Socket.IO Client** - Real-time communication
- **Marked.js** - Markdown rendering
- **Highlight.js** - Code syntax highlighting
- **Font Awesome** - Icon library

## ğŸ“Š Performance

- **Response Time**: < 2s for GPT-3.5, < 5s for local models
- **Streaming**: Real-time chunk delivery
- **Concurrent Users**: Supports multiple simultaneous connections
- **Storage**: JSON-based, scalable to thousands of conversations

## ğŸ”’ Security Notes

- API keys stored in environment variables (never in code)
- CORS enabled for development (configure for production)
- Input sanitization for XSS prevention
- Rate limiting recommended for production deployment

## ğŸš§ Future Enhancements

- [ ] User authentication and multi-user support
- [ ] Database integration (PostgreSQL/MongoDB)
- [ ] Voice input/output
- [ ] Image generation integration (DALL-E)
- [ ] Export conversations (PDF, Markdown, JSON)
- [ ] Custom system prompts
- [ ] Fine-tuned models
- [ ] Analytics dashboard
- [ ] Mobile app (React Native/Flutter)

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

## ğŸ“ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ‘¨â€ğŸ’» Author

**Rakesh Reddy Ravula**
- GitHub: [@rakeshreddyravula9-lgtm](https://github.com/rakeshreddyravula9-lgtm)
- Email: rakeshreddyravula9@gmail.com

## ğŸ™ Acknowledgments

- OpenAI for GPT API
- Hugging Face for transformer models
- Flask and Socket.IO communities
- Modern AI chat interface design

## ğŸ“§ Support

For issues, questions, or suggestions, please open an issue on GitHub.

---

**Built with â¤ï¸ using Python, Flask, and modern web technologies**
